{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "i5Jm3UN_Hfsu"
   },
   "source": [
    "## **Homework 4**\n",
    "**Instructions**\n",
    "* This homework focuses on understanding and applying CoCoOp for CLIP prompt tuning. It consists of **four questions** designed to assess both theoretical understanding and practical application.\n",
    "\n",
    "* Please organize your answers and results for the questions below and submit this jupyter notebook as **a .pdf file**.\n",
    "\n",
    "* **Deadline: 11/26 (Sat) 23:59**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QeRABv42Ku4E"
   },
   "source": [
    "### **Preparation**\n",
    "\n",
    "* Run the code below before proceeding with the homework (Q1, Q2).\n",
    "* If an error occurs, click ‘Run Session Again’ and then restart the runtime from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install learn2learn==0.1.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''!pip install learn2learn==0.1.5'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jNOsgBEzKucv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeongeun Park\\Downloads\\ProMetaR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ProMetaR' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeongeun Park\\Downloads\\ProMetaR\\Dassl.pytorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Dassl.pytorch' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flake8==3.7.9 in c:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 1)) (3.7.9)\n",
      "Requirement already satisfied: yapf==0.29.0 in c:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 2)) (0.29.0)\n",
      "Requirement already satisfied: isort==4.3.21 in c:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 3)) (4.3.21)\n",
      "Requirement already satisfied: yacs in c:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 4)) (0.1.8)\n",
      "Requirement already satisfied: gdown in c:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 5)) (5.2.0)\n",
      "Requirement already satisfied: tb-nightly in c:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 6)) (2.19.0a20241114)\n",
      "Requirement already satisfied: future in c:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: scipy in c:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 8)) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 9)) (1.4.2)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 10)) (4.66.4)\n",
      "Requirement already satisfied: ftfy in c:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 11)) (6.3.1)\n",
      "Requirement already satisfied: regex in c:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 12)) (2023.10.3)\n",
      "Requirement already satisfied: wilds==1.2.2 in c:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 13)) (1.2.2)\n",
      "Requirement already satisfied: tabulate in c:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 14)) (0.9.0)\n",
      "Requirement already satisfied: entrypoints<0.4.0,>=0.3.0 in c:\\anaconda\\lib\\site-packages (from flake8==3.7.9->-r requirements.txt (line 1)) (0.3)\n",
      "Requirement already satisfied: pyflakes<2.2.0,>=2.1.0 in c:\\anaconda\\lib\\site-packages (from flake8==3.7.9->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: pycodestyle<2.6.0,>=2.5.0 in c:\\anaconda\\lib\\site-packages (from flake8==3.7.9->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in c:\\anaconda\\lib\\site-packages (from flake8==3.7.9->-r requirements.txt (line 1)) (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.1 in c:\\anaconda\\lib\\site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (1.26.0)\n",
      "Requirement already satisfied: ogb>=1.2.6 in c:\\anaconda\\lib\\site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (1.3.6)\n",
      "Requirement already satisfied: outdated>=0.2.0 in c:\\anaconda\\lib\\site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (0.2.2)\n",
      "Requirement already satisfied: pandas>=1.1.0 in c:\\anaconda\\lib\\site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2.2.2)\n",
      "Requirement already satisfied: pillow>=7.2.0 in c:\\anaconda\\lib\\site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (10.3.0)\n",
      "Requirement already satisfied: pytz>=2020.4 in c:\\anaconda\\lib\\site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2024.1)\n",
      "Requirement already satisfied: torch>=1.7.0 in c:\\anaconda\\lib\\site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2.5.1)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in c:\\anaconda\\lib\\site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (0.20.1)\n",
      "Requirement already satisfied: PyYAML in c:\\anaconda\\lib\\site-packages (from yacs->-r requirements.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\anaconda\\lib\\site-packages (from gdown->-r requirements.txt (line 5)) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\anaconda\\lib\\site-packages (from gdown->-r requirements.txt (line 5)) (3.13.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\anaconda\\lib\\site-packages (from gdown->-r requirements.txt (line 5)) (2.32.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\anaconda\\lib\\site-packages (from tb-nightly->-r requirements.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\anaconda\\lib\\site-packages (from tb-nightly->-r requirements.txt (line 6)) (1.67.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\anaconda\\lib\\site-packages (from tb-nightly->-r requirements.txt (line 6)) (3.4.1)\n",
      "Requirement already satisfied: packaging in c:\\anaconda\\lib\\site-packages (from tb-nightly->-r requirements.txt (line 6)) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\anaconda\\lib\\site-packages (from tb-nightly->-r requirements.txt (line 6)) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\anaconda\\lib\\site-packages (from tb-nightly->-r requirements.txt (line 6)) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\anaconda\\lib\\site-packages (from tb-nightly->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\anaconda\\lib\\site-packages (from tb-nightly->-r requirements.txt (line 6)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\anaconda\\lib\\site-packages (from tb-nightly->-r requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\anaconda\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 9)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\anaconda\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 9)) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from tqdm->-r requirements.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: wcwidth in c:\\anaconda\\lib\\site-packages (from ftfy->-r requirements.txt (line 11)) (0.2.5)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in c:\\anaconda\\lib\\site-packages (from ogb>=1.2.6->wilds==1.2.2->-r requirements.txt (line 13)) (2.2.2)\n",
      "Requirement already satisfied: littleutils in c:\\anaconda\\lib\\site-packages (from outdated>=0.2.0->wilds==1.2.2->-r requirements.txt (line 13)) (0.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\anaconda\\lib\\site-packages (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13)) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\anaconda\\lib\\site-packages (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13)) (2023.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\anaconda\\lib\\site-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\anaconda\\lib\\site-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda\\lib\\site-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\anaconda\\lib\\site-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (2024.3.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\anaconda\\lib\\site-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda\\lib\\site-packages (from sympy==1.13.1->torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\anaconda\\lib\\site-packages (from werkzeug>=1.0.1->tb-nightly->-r requirements.txt (line 6)) (2.1.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\anaconda\\lib\\site-packages (from beautifulsoup4->gdown->-r requirements.txt (line 5)) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda\\lib\\site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\anaconda\\lib\\site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (1.7.1)\n",
      "C:\\Users\\Jeongeun Park\\Downloads\\ProMetaR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cp'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ftfy==6.1.1 (from -r requirements.txt (line 1))\n",
      "  Using cached ftfy-6.1.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: regex in c:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 2)) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\lib\\site-packages (from -r requirements.txt (line 3)) (4.66.4)\n",
      "Collecting learn2learn==0.2.0 (from -r requirements.txt (line 4))\n",
      "  Using cached learn2learn-0.2.0.tar.gz (7.0 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\anaconda\\lib\\site-packages (from ftfy==6.1.1->-r requirements.txt (line 1)) (0.2.5)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\anaconda\\lib\\site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (1.26.0)\n",
      "Collecting gym>=0.14.0 (from learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Using cached gym-0.26.2-py3-none-any.whl\n",
      "Requirement already satisfied: torch>=1.1.0 in c:\\anaconda\\lib\\site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (2.5.1)\n",
      "Requirement already satisfied: torchvision>=0.3.0 in c:\\anaconda\\lib\\site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (0.20.1)\n",
      "Requirement already satisfied: scipy in c:\\anaconda\\lib\\site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (1.13.1)\n",
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (2.32.2)\n",
      "Requirement already satisfied: gsutil in c:\\anaconda\\lib\\site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (5.31)\n",
      "Collecting qpth>=0.0.15 (from learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Using cached qpth-0.0.18-py3-none-any.whl\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from tqdm->-r requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\anaconda\\lib\\site-packages (from gym>=0.14.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.2.1)\n",
      "Collecting gym-notices>=0.0.4 (from gym>=0.14.0->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Using cached gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting cvxpy>=1.1.0 (from qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Using cached cvxpy-1.6.0-cp312-cp312-win_amd64.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\anaconda\\lib\\site-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\anaconda\\lib\\site-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\anaconda\\lib\\site-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda\\lib\\site-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\anaconda\\lib\\site-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda\\lib\\site-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\anaconda\\lib\\site-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda\\lib\\site-packages (from sympy==1.13.1->torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\anaconda\\lib\\site-packages (from torchvision>=0.3.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (10.3.0)\n",
      "Requirement already satisfied: argcomplete>=1.9.4 in c:\\anaconda\\lib\\site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.5.1)\n",
      "Requirement already satisfied: crcmod>=1.7 in c:\\anaconda\\lib\\site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.7)\n",
      "Requirement already satisfied: fasteners>=0.14.1 in c:\\anaconda\\lib\\site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.19)\n",
      "Requirement already satisfied: gcs-oauth2-boto-plugin>=3.2 in c:\\anaconda\\lib\\site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.2)\n",
      "Requirement already satisfied: google-apitools>=0.5.32 in c:\\anaconda\\lib\\site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.5.32)\n",
      "Requirement already satisfied: httplib2==0.20.4 in c:\\anaconda\\lib\\site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.20.4)\n",
      "Requirement already satisfied: google-reauth>=0.1.0 in c:\\anaconda\\lib\\site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.1.1)\n",
      "Requirement already satisfied: monotonic>=1.4 in c:\\anaconda\\lib\\site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.6)\n",
      "Requirement already satisfied: pyOpenSSL>=0.13 in c:\\anaconda\\lib\\site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (24.0.0)\n",
      "Requirement already satisfied: retry-decorator>=1.0.0 in c:\\anaconda\\lib\\site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.1.1)\n",
      "Requirement already satisfied: six>=1.16.0 in c:\\anaconda\\lib\\site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: google-auth==2.17.0 in c:\\anaconda\\lib\\site-packages (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.17.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.2.0 in c:\\anaconda\\lib\\site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\anaconda\\lib\\site-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\anaconda\\lib\\site-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\anaconda\\lib\\site-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (4.7.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0dev,>=3.6.2 in c:\\anaconda\\lib\\site-packages (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.9.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\anaconda\\lib\\site-packages (from httplib2==0.20.4->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda\\lib\\site-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (2024.8.30)\n",
      "Collecting osqp>=0.6.2 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Using cached osqp-0.6.7.post3-cp312-cp312-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting clarabel>=0.5.0 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Using cached clarabel-0.9.0-cp37-abi3-win_amd64.whl.metadata (4.8 kB)\n",
      "Collecting scs>=3.2.4.post1 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Using cached scs-3.2.7-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: boto>=2.29.1 in c:\\anaconda\\lib\\site-packages (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.49.0)\n",
      "Requirement already satisfied: oauth2client>=2.2.0 in c:\\anaconda\\lib\\site-packages (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (4.1.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\anaconda\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.4.8)\n",
      "Requirement already satisfied: pyu2f in c:\\anaconda\\lib\\site-packages (from google-reauth>=0.1.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.1.5)\n",
      "Requirement already satisfied: cryptography<43,>=41.0.5 in c:\\anaconda\\lib\\site-packages (from pyOpenSSL>=0.13->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (42.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda\\lib\\site-packages (from jinja2->torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.1.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\anaconda\\lib\\site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\anaconda\\lib\\site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\anaconda\\lib\\site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\anaconda\\lib\\site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\anaconda\\lib\\site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.9.3)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\anaconda\\lib\\site-packages (from cryptography<43,>=41.0.5->pyOpenSSL>=0.13->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.16.0)\n",
      "Collecting qdldl (from osqp>=0.6.2->cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Using cached qdldl-0.1.7.post4-cp312-cp312-win_amd64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pycparser in c:\\anaconda\\lib\\site-packages (from cffi>=1.12->cryptography<43,>=41.0.5->pyOpenSSL>=0.13->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.21)\n",
      "Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Using cached cvxpy-1.6.0-cp312-cp312-win_amd64.whl (1.1 MB)\n",
      "Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Using cached clarabel-0.9.0-cp37-abi3-win_amd64.whl (736 kB)\n",
      "Using cached osqp-0.6.7.post3-cp312-cp312-win_amd64.whl (293 kB)\n",
      "Using cached scs-3.2.7-cp312-cp312-win_amd64.whl (8.4 MB)\n",
      "Using cached qdldl-0.1.7.post4-cp312-cp312-win_amd64.whl (87 kB)\n",
      "Building wheels for collected packages: learn2learn\n",
      "  Building wheel for learn2learn (setup.py): started\n",
      "  Building wheel for learn2learn (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for learn2learn\n",
      "Failed to build learn2learn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [165 lines of output]\n",
      "  C:\\anaconda\\Lib\\site-packages\\setuptools\\__init__.py:94: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Requirements should be satisfied by a PEP 517 installer.\n",
      "          If you are using pip, you can try `pip install --use-pep517`.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    dist.fetch_build_eggs(dist.setup_requires)\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\n",
      "  copying learn2learn\\_version.py -> build\\lib.win-amd64-cpython-312\\learn2learn\n",
      "  copying learn2learn\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\n",
      "  creating build\\lib.win-amd64-cpython-312\\tests\n",
      "  copying tests\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\n",
      "  copying learn2learn\\algorithms\\base_learner.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\n",
      "  copying learn2learn\\algorithms\\gbml.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\n",
      "  copying learn2learn\\algorithms\\maml.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\n",
      "  copying learn2learn\\algorithms\\meta_sgd.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\n",
      "  copying learn2learn\\algorithms\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\data\n",
      "  copying learn2learn\\data\\samplers.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\data\n",
      "  copying learn2learn\\data\\utils.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\data\n",
      "  copying learn2learn\\data\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\data\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\gym\n",
      "  copying learn2learn\\gym\\async_vec_env.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\n",
      "  copying learn2learn\\gym\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\nn\n",
      "  copying learn2learn\\nn\\kroneckers.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\n",
      "  copying learn2learn\\nn\\metaoptnet.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\n",
      "  copying learn2learn\\nn\\misc.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\n",
      "  copying learn2learn\\nn\\protonet.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\n",
      "  copying learn2learn\\nn\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\optim\n",
      "  copying learn2learn\\optim\\learnable_optimizer.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\n",
      "  copying learn2learn\\optim\\parameter_update.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\n",
      "  copying learn2learn\\optim\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\text\n",
      "  copying learn2learn\\text\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\text\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\utils\n",
      "  copying learn2learn\\utils\\lightning.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\utils\n",
      "  copying learn2learn\\utils\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\utils\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\vision\n",
      "  copying learn2learn\\vision\\transforms.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\n",
      "  copying learn2learn\\vision\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
      "  copying learn2learn\\algorithms\\lightning\\lightning_anil.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
      "  copying learn2learn\\algorithms\\lightning\\lightning_episodic_module.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
      "  copying learn2learn\\algorithms\\lightning\\lightning_maml.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
      "  copying learn2learn\\algorithms\\lightning\\lightning_metaoptnet.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
      "  copying learn2learn\\algorithms\\lightning\\lightning_protonet.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
      "  copying learn2learn\\algorithms\\lightning\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\algorithms\\lightning\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\n",
      "  copying learn2learn\\gym\\envs\\meta_env.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\n",
      "  copying learn2learn\\gym\\envs\\subproc_vec_env.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\n",
      "  copying learn2learn\\gym\\envs\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\metaworld\n",
      "  copying learn2learn\\gym\\envs\\metaworld\\metaworld.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\metaworld\n",
      "  copying learn2learn\\gym\\envs\\metaworld\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\metaworld\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
      "  copying learn2learn\\gym\\envs\\mujoco\\ant_direction.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
      "  copying learn2learn\\gym\\envs\\mujoco\\ant_forward_backward.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
      "  copying learn2learn\\gym\\envs\\mujoco\\dummy_mujoco_env.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
      "  copying learn2learn\\gym\\envs\\mujoco\\halfcheetah_forward_backward.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
      "  copying learn2learn\\gym\\envs\\mujoco\\humanoid_direction.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
      "  copying learn2learn\\gym\\envs\\mujoco\\humanoid_forward_backward.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
      "  copying learn2learn\\gym\\envs\\mujoco\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\mujoco\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\particles\n",
      "  copying learn2learn\\gym\\envs\\particles\\particles_2d.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\particles\n",
      "  copying learn2learn\\gym\\envs\\particles\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\gym\\envs\\particles\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\nn\\metalayers\n",
      "  copying learn2learn\\nn\\metalayers\\metamodule.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\\metalayers\n",
      "  copying learn2learn\\nn\\metalayers\\parameter_transform.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\\metalayers\n",
      "  copying learn2learn\\nn\\metalayers\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\nn\\metalayers\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\transforms\n",
      "  copying learn2learn\\optim\\transforms\\kronecker_transform.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\transforms\n",
      "  copying learn2learn\\optim\\transforms\\metacurvature_transform.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\transforms\n",
      "  copying learn2learn\\optim\\transforms\\module_transform.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\transforms\n",
      "  copying learn2learn\\optim\\transforms\\transform_dictionary.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\transforms\n",
      "  copying learn2learn\\optim\\transforms\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\transforms\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\update_rules\n",
      "  copying learn2learn\\optim\\update_rules\\differentiable_sgd.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\update_rules\n",
      "  copying learn2learn\\optim\\update_rules\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\optim\\update_rules\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\text\\datasets\n",
      "  copying learn2learn\\text\\datasets\\news_classification.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\text\\datasets\n",
      "  copying learn2learn\\text\\datasets\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\text\\datasets\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
      "  copying learn2learn\\vision\\benchmarks\\cifarfs_benchmark.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
      "  copying learn2learn\\vision\\benchmarks\\fc100_benchmark.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
      "  copying learn2learn\\vision\\benchmarks\\mini_imagenet_benchmark.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
      "  copying learn2learn\\vision\\benchmarks\\omniglot_benchmark.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
      "  copying learn2learn\\vision\\benchmarks\\tiered_imagenet_benchmark.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
      "  copying learn2learn\\vision\\benchmarks\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\benchmarks\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
      "  copying learn2learn\\vision\\datasets\\cifarfs.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
      "  copying learn2learn\\vision\\datasets\\cu_birds200.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
      "  copying learn2learn\\vision\\datasets\\describable_textures.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
      "  copying learn2learn\\vision\\datasets\\fc100.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
      "  copying learn2learn\\vision\\datasets\\fgvc_aircraft.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
      "  copying learn2learn\\vision\\datasets\\fgvc_fungi.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
      "  copying learn2learn\\vision\\datasets\\full_omniglot.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
      "  copying learn2learn\\vision\\datasets\\mini_imagenet.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
      "  copying learn2learn\\vision\\datasets\\quickdraw.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
      "  copying learn2learn\\vision\\datasets\\tiered_imagenet.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
      "  copying learn2learn\\vision\\datasets\\vgg_flowers.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
      "  copying learn2learn\\vision\\datasets\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\datasets\n",
      "  creating build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\models\n",
      "  copying learn2learn\\vision\\models\\cnn4.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\models\n",
      "  copying learn2learn\\vision\\models\\resnet12.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\models\n",
      "  copying learn2learn\\vision\\models\\wrn28.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\models\n",
      "  copying learn2learn\\vision\\models\\__init__.py -> build\\lib.win-amd64-cpython-312\\learn2learn\\vision\\models\n",
      "  creating build\\lib.win-amd64-cpython-312\\tests\\integration\n",
      "  copying tests\\integration\\maml_miniimagenet_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\integration\n",
      "  copying tests\\integration\\maml_omniglot_test.py -> build\\lib.win-amd64-cpython-312\\tests\\integration\n",
      "  copying tests\\integration\\protonets_miniimagenet_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\integration\n",
      "  copying tests\\integration\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\\integration\n",
      "  creating build\\lib.win-amd64-cpython-312\\tests\\unit\n",
      "  copying tests\\unit\\utils_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\n",
      "  copying tests\\unit\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\n",
      "  creating build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
      "  copying tests\\unit\\algorithms\\gbml_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
      "  copying tests\\unit\\algorithms\\lightning_anil_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
      "  copying tests\\unit\\algorithms\\lightning_maml_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
      "  copying tests\\unit\\algorithms\\lightning_metaoptnet_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
      "  copying tests\\unit\\algorithms\\lightning_protonet_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
      "  copying tests\\unit\\algorithms\\maml_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
      "  copying tests\\unit\\algorithms\\metasgd_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
      "  copying tests\\unit\\algorithms\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\algorithms\n",
      "  creating build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
      "  copying tests\\unit\\data\\metadataset_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
      "  copying tests\\unit\\data\\task_dataset_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
      "  copying tests\\unit\\data\\transforms_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
      "  copying tests\\unit\\data\\utils_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
      "  copying tests\\unit\\data\\util_datasets.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
      "  copying tests\\unit\\data\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\data\n",
      "  creating build\\lib.win-amd64-cpython-312\\tests\\unit\\nn\n",
      "  copying tests\\unit\\nn\\kroneckers_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\nn\n",
      "  copying tests\\unit\\nn\\metaoptnet_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\nn\n",
      "  copying tests\\unit\\nn\\misc.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\nn\n",
      "  copying tests\\unit\\nn\\protonet_test.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\nn\n",
      "  copying tests\\unit\\nn\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\nn\n",
      "  creating build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
      "  copying tests\\unit\\vision\\benchmarks_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
      "  copying tests\\unit\\vision\\cifarfs_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
      "  copying tests\\unit\\vision\\cu_birds200_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
      "  copying tests\\unit\\vision\\describable_textures_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
      "  copying tests\\unit\\vision\\fc100_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
      "  copying tests\\unit\\vision\\fgvc_aircraft_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
      "  copying tests\\unit\\vision\\pretrained_backbones_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
      "  copying tests\\unit\\vision\\quickdraw_test_no.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
      "  copying tests\\unit\\vision\\tiered_imagenet_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
      "  copying tests\\unit\\vision\\transform_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
      "  copying tests\\unit\\vision\\vgg_flowers_test_notravis.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
      "  copying tests\\unit\\vision\\__init__.py -> build\\lib.win-amd64-cpython-312\\tests\\unit\\vision\n",
      "  running build_ext\n",
      "  building 'learn2learn.data.meta_dataset' extension\n",
      "  creating build\\temp.win-amd64-cpython-312\\Release\\learn2learn\\data\n",
      "  \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.41.34120\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\\anaconda\\include -IC:\\anaconda\\Include \"-IC:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.41.34120\\include\" \"-IC:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.41.34120\\ATLMFC\\include\" \"-IC:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" /Tclearn2learn/data/meta_dataset.c /Fobuild\\temp.win-amd64-cpython-312\\Release\\learn2learn/data/meta_dataset.obj\n",
      "  meta_dataset.c\n",
      "  learn2learn/data/meta_dataset.c(210): fatal error C1083: 포함 파일을 열 수 없습니다. 'longintrepr.h': No such file or directory\n",
      "  error: command 'C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.41.34120\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for learn2learn\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (learn2learn)\n",
      "하위 디렉터리 또는 파일 outputs이(가) 이미 있습니다.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeongeun Park\\Downloads\\ProMetaR\\data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "하위 디렉터리 또는 파일 data이(가) 이미 있습니다.\n",
      "하위 디렉터리 또는 파일 eurosat이(가) 이미 있습니다.\n",
      "'wget'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeongeun Park\\Downloads\\ProMetaR\\data\\eurosat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Ip7yaCWFi0eaOFUGga0lUdVi_DDQth1o\n",
      "To: C:\\Users\\Jeongeun Park\\Downloads\\ProMetaR\\data\\eurosat\\split_zhou_EuroSAT.json\n",
      "\n",
      "  0%|          | 0.00/3.01M [00:00<?, ?B/s]\n",
      " 17%|#7        | 524k/3.01M [00:00<00:00, 3.42MB/s]\n",
      " 52%|#####2    | 1.57M/3.01M [00:00<00:00, 4.89MB/s]\n",
      "100%|##########| 3.01M/3.01M [00:00<00:00, 7.39MB/s]\n",
      "100%|##########| 3.01M/3.01M [00:00<00:00, 6.51MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeongeun Park\\Downloads\\ProMetaR\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mlvlab/ProMetaR.git\n",
    "%cd ProMetaR/\n",
    "\n",
    "!git clone https://github.com/KaiyangZhou/Dassl.pytorch.git\n",
    "%cd Dassl.pytorch/\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -r requirements.txt\n",
    "!cp -r dassl ../\n",
    "# Install this library (no need to re-build if the source code is modified)\n",
    "# !python setup.py develop\n",
    "%cd ..\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "%mkdir outputs\n",
    "%mkdir data\n",
    "\n",
    "%cd data\n",
    "%mkdir eurosat\n",
    "!wget http://madm.dfki.de/files/sentinel/EuroSAT.zip EuroSAT.zip\n",
    "\n",
    "!unzip -o EuroSAT.zip -d eurosat/\n",
    "%cd eurosat\n",
    "!gdown 1Ip7yaCWFi0eaOFUGga0lUdVi_DDQth1o\n",
    "\n",
    "%cd ../../\n",
    "\n",
    "\n",
    "import os.path as osp\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from clip import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import argparse\n",
    "from dassl.utils import setup_logger, set_random_seed, collect_env_info\n",
    "from dassl.config import get_cfg_default\n",
    "from dassl.engine import build_trainer\n",
    "from dassl.engine import TRAINER_REGISTRY, TrainerX\n",
    "from dassl.metrics import compute_accuracy\n",
    "from dassl.utils import load_pretrained_weights, load_checkpoint\n",
    "from dassl.optim import build_optimizer, build_lr_scheduler\n",
    "\n",
    "# custom\n",
    "import datasets.oxford_pets\n",
    "import datasets.oxford_flowers\n",
    "import datasets.fgvc_aircraft\n",
    "import datasets.dtd\n",
    "import datasets.eurosat\n",
    "import datasets.stanford_cars\n",
    "import datasets.food101\n",
    "import datasets.sun397\n",
    "import datasets.caltech101\n",
    "import datasets.ucf101\n",
    "import datasets.imagenet\n",
    "import datasets.imagenet_sketch\n",
    "import datasets.imagenetv2\n",
    "import datasets.imagenet_a\n",
    "import datasets.imagenet_r\n",
    "\n",
    "def print_args(args, cfg):\n",
    "    print(\"***************\")\n",
    "    print(\"** Arguments **\")\n",
    "    print(\"***************\")\n",
    "    optkeys = list(args.__dict__.keys())\n",
    "    optkeys.sort()\n",
    "    for key in optkeys:\n",
    "        print(\"{}: {}\".format(key, args.__dict__[key]))\n",
    "    print(\"************\")\n",
    "    print(\"** Config **\")\n",
    "    print(\"************\")\n",
    "    print(cfg)\n",
    "\n",
    "def reset_cfg(cfg, args):\n",
    "    if args.root:\n",
    "        cfg.DATASET.ROOT = args.root\n",
    "    if args.output_dir:\n",
    "        cfg.OUTPUT_DIR = args.output_dir\n",
    "    if args.seed:\n",
    "        cfg.SEED = args.seed\n",
    "    if args.trainer:\n",
    "        cfg.TRAINER.NAME = args.trainer\n",
    "    cfg.DATASET.NUM_SHOTS = 16\n",
    "    cfg.DATASET.SUBSAMPLE_CLASSES = args.subsample_classes\n",
    "    cfg.DATALOADER.TRAIN_X.BATCH_SIZE = args.train_batch_size\n",
    "    cfg.OPTIM.MAX_EPOCH = args.epoch\n",
    "\n",
    "def extend_cfg(cfg):\n",
    "    \"\"\"\n",
    "    Add new config variables.\n",
    "    \"\"\"\n",
    "    from yacs.config import CfgNode as CN\n",
    "    cfg.TRAINER.COOP = CN()\n",
    "    cfg.TRAINER.COOP.N_CTX = 16  # number of context vectors\n",
    "    cfg.TRAINER.COOP.CSC = False  # class-specific context\n",
    "    cfg.TRAINER.COOP.CTX_INIT = \"\"  # initialization words\n",
    "    cfg.TRAINER.COOP.PREC = \"fp16\"  # fp16, fp32, amp\n",
    "    cfg.TRAINER.COOP.CLASS_TOKEN_POSITION = \"end\"  # 'middle' or 'end' or 'front'\n",
    "    cfg.TRAINER.COCOOP = CN()\n",
    "    cfg.TRAINER.COCOOP.N_CTX = 4  # number of context vectors\n",
    "    cfg.TRAINER.COCOOP.CTX_INIT = \"a photo of a\"  # initialization words\n",
    "    cfg.TRAINER.COCOOP.PREC = \"fp16\"  # fp16, fp32, amp\n",
    "    cfg.TRAINER.PROMETAR = CN()\n",
    "    cfg.TRAINER.PROMETAR.N_CTX_VISION = 4  # number of context vectors at the vision branch\n",
    "    cfg.TRAINER.PROMETAR.N_CTX_TEXT = 4  # number of context vectors at the language branch\n",
    "    cfg.TRAINER.PROMETAR.CTX_INIT = \"a photo of a\"  # initialization words\n",
    "    cfg.TRAINER.PROMETAR.PREC = \"fp16\"  # fp16, fp32, amp\n",
    "    cfg.TRAINER.PROMETAR.PROMPT_DEPTH_VISION = 9  # Max 12, minimum 0, for 0 it will be using shallow IVLP prompting (J=1)\n",
    "    cfg.TRAINER.PROMETAR.PROMPT_DEPTH_TEXT = 9  # Max 12, minimum 0, for 0 it will be using shallow IVLP prompting (J=1)\n",
    "    cfg.DATASET.SUBSAMPLE_CLASSES = \"all\"  # all, base or new\n",
    "    cfg.TRAINER.PROMETAR.ADAPT_LR = 0.0005\n",
    "    cfg.TRAINER.PROMETAR.LR_RATIO = 0.0005\n",
    "    cfg.TRAINER.PROMETAR.FAST_ADAPTATION = False\n",
    "    cfg.TRAINER.PROMETAR.MIXUP_ALPHA = 0.5\n",
    "    cfg.TRAINER.PROMETAR.MIXUP_BETA = 0.5\n",
    "    cfg.TRAINER.PROMETAR.DIM_RATE=8\n",
    "    cfg.OPTIM_VNET = CN()\n",
    "    cfg.OPTIM_VNET.NAME = \"adam\"\n",
    "    cfg.OPTIM_VNET.LR = 0.0003\n",
    "    cfg.OPTIM_VNET.WEIGHT_DECAY = 5e-4\n",
    "    cfg.OPTIM_VNET.MOMENTUM = 0.9\n",
    "    cfg.OPTIM_VNET.SGD_DAMPNING = 0\n",
    "    cfg.OPTIM_VNET.SGD_NESTEROV = False\n",
    "    cfg.OPTIM_VNET.RMSPROP_ALPHA = 0.99\n",
    "    cfg.OPTIM_VNET.ADAM_BETA1 = 0.9\n",
    "    cfg.OPTIM_VNET.ADAM_BETA2 = 0.999\n",
    "    cfg.OPTIM_VNET.STAGED_LR = False\n",
    "    cfg.OPTIM_VNET.NEW_LAYERS = ()\n",
    "    cfg.OPTIM_VNET.BASE_LR_MULT = 0.1\n",
    "    # Learning rate scheduler\n",
    "    cfg.OPTIM_VNET.LR_SCHEDULER = \"single_step\"\n",
    "    # -1 or 0 means the stepsize is equal to max_epoch\n",
    "    cfg.OPTIM_VNET.STEPSIZE = (-1, )\n",
    "    cfg.OPTIM_VNET.GAMMA = 0.1\n",
    "    cfg.OPTIM_VNET.MAX_EPOCH = 10\n",
    "    # Set WARMUP_EPOCH larger than 0 to activate warmup training\n",
    "    cfg.OPTIM_VNET.WARMUP_EPOCH = -1\n",
    "    # Either linear or constant\n",
    "    cfg.OPTIM_VNET.WARMUP_TYPE = \"linear\"\n",
    "    # Constant learning rate when type=constant\n",
    "    cfg.OPTIM_VNET.WARMUP_CONS_LR = 1e-5\n",
    "    # Minimum learning rate when type=linear\n",
    "    cfg.OPTIM_VNET.WARMUP_MIN_LR = 1e-5\n",
    "    # Recount epoch for the next scheduler (last_epoch=-1)\n",
    "    # Otherwise last_epoch=warmup_epoch\n",
    "    cfg.OPTIM_VNET.WARMUP_RECOUNT = True\n",
    "\n",
    "def setup_cfg(args):\n",
    "    cfg = get_cfg_default()\n",
    "    extend_cfg(cfg)\n",
    "    # 1. From the dataset config file\n",
    "    if args.dataset_config_file:\n",
    "        cfg.merge_from_file(args.dataset_config_file)\n",
    "    # 2. From the method config file\n",
    "    if args.config_file:\n",
    "        cfg.merge_from_file(args.config_file)\n",
    "    # 3. From input arguments\n",
    "    reset_cfg(cfg, args)\n",
    "    cfg.freeze()\n",
    "    return cfg\n",
    "\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def load_clip_to_cpu(cfg): # Load CLIP\n",
    "    backbone_name = cfg.MODEL.BACKBONE.NAME\n",
    "    url = clip._MODELS[backbone_name]\n",
    "    model_path = clip._download(url)\n",
    "\n",
    "    try:\n",
    "        # loading JIT archive\n",
    "        model = torch.jit.load(model_path, map_location=\"cpu\").eval()\n",
    "        state_dict = None\n",
    "\n",
    "    except RuntimeError:\n",
    "        state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "\n",
    "    if cfg.TRAINER.NAME == \"\":\n",
    "      design_trainer = \"CoOp\"\n",
    "    else:\n",
    "      design_trainer = cfg.TRAINER.NAME\n",
    "    design_details = {\"trainer\": design_trainer,\n",
    "                      \"vision_depth\": 0,\n",
    "                      \"language_depth\": 0, \"vision_ctx\": 0,\n",
    "                      \"language_ctx\": 0}\n",
    "    model = clip.build_model(state_dict or model.state_dict(), design_details)\n",
    "\n",
    "    return model\n",
    "\n",
    "from dassl.config import get_cfg_default\n",
    "cfg = get_cfg_default()\n",
    "cfg.MODEL.BACKBONE.NAME = \"ViT-B/16\" # Set the vision encoder backbone of CLIP to ViT.\n",
    "clip_model = load_clip_to_cpu(cfg)\n",
    "\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model): # 초기화 하는 함수\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts): # 모델 호출\n",
    "        x = prompts + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "@TRAINER_REGISTRY.register(force=True)\n",
    "class CoCoOp(TrainerX):\n",
    "    def check_cfg(self, cfg):\n",
    "        assert cfg.TRAINER.COCOOP.PREC in [\"fp16\", \"fp32\", \"amp\"]\n",
    "\n",
    "    def build_model(self):\n",
    "        cfg = self.cfg\n",
    "        classnames = self.dm.dataset.classnames\n",
    "        print(f\"Loading CLIP (backbone: {cfg.MODEL.BACKBONE.NAME})\")\n",
    "        clip_model = load_clip_to_cpu(cfg)\n",
    "\n",
    "        if cfg.TRAINER.COCOOP.PREC == \"fp32\" or cfg.TRAINER.COCOOP.PREC == \"amp\":\n",
    "            # CLIP's default precision is fp16\n",
    "            clip_model.float()\n",
    "\n",
    "        print(\"Building custom CLIP\")\n",
    "        self.model = CoCoOpCustomCLIP(cfg, classnames, clip_model)\n",
    "\n",
    "        print(\"Turning off gradients in both the image and the text encoder\")\n",
    "        name_to_update = \"prompt_learner\"\n",
    "\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name_to_update not in name:\n",
    "                param.requires_grad_(False)\n",
    "\n",
    "        # Double check\n",
    "        enabled = set()\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                enabled.add(name)\n",
    "        print(f\"Parameters to be updated: {enabled}\")\n",
    "\n",
    "        if cfg.MODEL.INIT_WEIGHTS:\n",
    "            load_pretrained_weights(self.model.prompt_learner, cfg.MODEL.INIT_WEIGHTS)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        # NOTE: only give prompt_learner to the optimizer\n",
    "        self.optim = build_optimizer(self.model.prompt_learner, cfg.OPTIM)\n",
    "        self.sched = build_lr_scheduler(self.optim, cfg.OPTIM)\n",
    "        self.register_model(\"prompt_learner\", self.model.prompt_learner, self.optim, self.sched)\n",
    "\n",
    "        self.scaler = GradScaler() if cfg.TRAINER.COCOOP.PREC == \"amp\" else None\n",
    "\n",
    "        # Note that multi-gpu training could be slow because CLIP's size is\n",
    "        # big, which slows down the copy operation in DataParallel\n",
    "        device_count = torch.cuda.device_count()\n",
    "        if device_count > 1:\n",
    "            print(f\"Multiple GPUs detected (n_gpus={device_count}), use all of them!\")\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "\n",
    "    def before_train(self):\n",
    "        directory = self.cfg.OUTPUT_DIR\n",
    "        if self.cfg.RESUME:\n",
    "            directory = self.cfg.RESUME\n",
    "        self.start_epoch = self.resume_model_if_exist(directory)\n",
    "\n",
    "        # Remember the starting time (for computing the elapsed time)\n",
    "        self.time_start = time.time()\n",
    "\n",
    "\n",
    "    def forward_backward(self, batch):\n",
    "        image, label = self.parse_batch_train(batch)\n",
    "\n",
    "        model = self.model\n",
    "        optim = self.optim\n",
    "        scaler = self.scaler\n",
    "\n",
    "        prec = self.cfg.TRAINER.COCOOP.PREC\n",
    "        loss = model(image, label) # Input image 모델 통과\n",
    "        optim.zero_grad()\n",
    "        loss.backward() # Backward (역전파)\n",
    "        optim.step() # 모델 parameter update\n",
    "\n",
    "        loss_summary = {\"loss\": loss.item()}\n",
    "\n",
    "        if (self.batch_idx + 1) == self.num_batches:\n",
    "            self.update_lr()\n",
    "\n",
    "        return loss_summary\n",
    "\n",
    "    def parse_batch_train(self, batch):\n",
    "        input = batch[\"img\"]\n",
    "        label = batch[\"label\"]\n",
    "        input = input.to(self.device)\n",
    "        label = label.to(self.device)\n",
    "        return input, label\n",
    "\n",
    "    def load_model(self, directory, epoch=None):\n",
    "        if not directory:\n",
    "            print(\"Note that load_model() is skipped as no pretrained model is given\")\n",
    "            return\n",
    "\n",
    "        names = self.get_model_names()\n",
    "\n",
    "        # By default, the best model is loaded\n",
    "        model_file = \"model-best.pth.tar\"\n",
    "\n",
    "        if epoch is not None:\n",
    "            model_file = \"model.pth.tar-\" + str(epoch)\n",
    "\n",
    "        for name in names:\n",
    "            model_path = osp.join(directory, name, model_file)\n",
    "\n",
    "            if not osp.exists(model_path):\n",
    "                raise FileNotFoundError('Model not found at \"{}\"'.format(model_path))\n",
    "\n",
    "            checkpoint = load_checkpoint(model_path)\n",
    "            state_dict = checkpoint[\"state_dict\"]\n",
    "            epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "            # Ignore fixed token vectors\n",
    "            if \"token_prefix\" in state_dict:\n",
    "                del state_dict[\"token_prefix\"]\n",
    "\n",
    "            if \"token_suffix\" in state_dict:\n",
    "                del state_dict[\"token_suffix\"]\n",
    "\n",
    "            print(\"Loading weights to {} \" 'from \"{}\" (epoch = {})'.format(name, model_path, epoch))\n",
    "            # set strict=False\n",
    "            self._models[name].load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    def after_train(self):\n",
    "      print(\"Finish training\")\n",
    "\n",
    "      do_test = not self.cfg.TEST.NO_TEST\n",
    "      if do_test:\n",
    "          if self.cfg.TEST.FINAL_MODEL == \"best_val\":\n",
    "              print(\"Deploy the model with the best val performance\")\n",
    "              self.load_model(self.output_dir)\n",
    "          else:\n",
    "              print(\"Deploy the last-epoch model\")\n",
    "          acc = self.test()\n",
    "\n",
    "      # Show elapsed time\n",
    "      elapsed = round(time.time() - self.time_start)\n",
    "      elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "      print(f\"Elapsed: {elapsed}\")\n",
    "\n",
    "      # Close writer\n",
    "      self.close_writer()\n",
    "      return acc\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Generic training loops.\"\"\"\n",
    "        self.before_train()\n",
    "        for self.epoch in range(self.start_epoch, self.max_epoch):\n",
    "            self.before_epoch()\n",
    "            self.run_epoch()\n",
    "            self.after_epoch()\n",
    "        acc = self.after_train()\n",
    "        return acc\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--root\", type=str, default=\"data/\", help=\"path to dataset\")\n",
    "parser.add_argument(\"--output-dir\", type=str, default=\"outputs/cocoop3\", help=\"output directory\")\n",
    "parser.add_argument(\n",
    "    \"--seed\", type=int, default=1, help=\"only positive value enables a fixed seed\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--config-file\", type=str, default=\"configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\", help=\"path to config file\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset-config-file\",\n",
    "    type=str,\n",
    "    default=\"configs/datasets/eurosat.yaml\",\n",
    "    help=\"path to config file for dataset setup\",\n",
    ")\n",
    "parser.add_argument(\"--trainer\", type=str, default=\"CoOp\", help=\"name of trainer\")\n",
    "parser.add_argument(\"--eval-only\", action=\"store_true\", help=\"evaluation only\")\n",
    "parser.add_argument(\n",
    "    \"--model-dir\",\n",
    "    type=str,\n",
    "    default=\"\",\n",
    "    help=\"load model from this directory for eval-only mode\",\n",
    ")\n",
    "parser.add_argument(\"--train-batch-size\", type=int, default=4)\n",
    "parser.add_argument(\"--epoch\", type=int, default=10)\n",
    "parser.add_argument(\"--subsample-classes\", type=str, default=\"base\")\n",
    "parser.add_argument(\n",
    "    \"--load-epoch\", type=int, default=0, help=\"load model weights at this epoch for evaluation\"\n",
    ")\n",
    "args = parser.parse_args([])\n",
    "\n",
    "def main(args):\n",
    "    cfg = setup_cfg(args)\n",
    "    if cfg.SEED >= 0:\n",
    "        set_random_seed(cfg.SEED)\n",
    "\n",
    "    if torch.cuda.is_available() and cfg.USE_CUDA:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    trainer = build_trainer(cfg)\n",
    "    if args.eval_only:\n",
    "        trainer.load_model(args.model_dir, epoch=args.load_epoch)\n",
    "        acc = trainer.test()\n",
    "        return acc\n",
    "\n",
    "    acc = trainer.train()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# pip 설치\\n!python -m ensurepip\\n\\n# pip 업그레이드\\n!python -m pip install --upgrade pip'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# pip 설치\n",
    "!python -m ensurepip\n",
    "\n",
    "# pip 업그레이드\n",
    "!python -m pip install --upgrade pip'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "G3n9blo4JO7m"
   },
   "source": [
    "### **Q1.  Understanding and implementing CoCoOp**\n",
    "* We have learned how to define CoOp in Lab Session 4.\n",
    "\n",
    "* The main difference between CoOp and CoCoOp is **meta network** to extract image tokens that is added to the text prompt.\n",
    "\n",
    "* Based on the CoOp code given in Lab Session 4, fill-in-the-blank exercise to test your understanding of critical parts of the CoCoOp.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SONlVIhPH_qF"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CoCoOpPromptLearner(nn.Module):\n",
    "    def __init__(self, cfg, classnames, clip_model):\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        n_ctx = cfg.TRAINER.COCOOP.N_CTX\n",
    "        ctx_init = cfg.TRAINER.COCOOP.CTX_INIT\n",
    "        dtype = clip_model.dtype\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        vis_dim = clip_model.visual.output_dim\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "        cfg_imsize = cfg.INPUT.SIZE[0]\n",
    "        assert cfg_imsize == clip_imsize, f\"cfg_imsize ({cfg_imsize}) must equal to clip_imsize ({clip_imsize})\"\n",
    "\n",
    "        if ctx_init:\n",
    "            # use given words to initialize context vectors\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
    "            ctx_vectors = embedding[0, 1: 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            # random initialization\n",
    "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "        print(f'Initial context: \"{prompt_prefix}\"')\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        self.ctx = nn.Parameter(ctx_vectors)  # Wrap the initialized prompts above as parameters to make them trainable.\n",
    "\n",
    "        ### Tokenize ###\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]  # 예) \"Forest\"\n",
    "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames] # 예) \"A photo of Forest.\"\n",
    "\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]) # 예) [49406, 320, 1125, 539...]\n",
    "\n",
    "\n",
    "\n",
    "        #####################################\n",
    "        ####### Q1. Fill in the blank #######\n",
    "        ########## Define Meta Net ##########\n",
    "        self.meta_net = nn.Sequential(OrderedDict([\n",
    "            #(\"linear1\", \"fill in here\"(vis_dim, vis_dim // 16)),\n",
    "            (\"linear1\", nn.Linear(vis_dim, vis_dim // 16)),\n",
    "            (\"relu\", nn.ReLU(inplace=True)),\n",
    "            (\"linear2\", nn.Linear(vis_dim // 16, ctx_dim))\n",
    "        ]))\n",
    "        #####################################\n",
    "        ## Hint: meta network is composed to linear layer, relu activation, and linear layer.\n",
    "\n",
    "\n",
    "\n",
    "        if cfg.TRAINER.COCOOP.PREC == \"fp16\":\n",
    "            self.meta_net.half()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts).type(dtype)\n",
    "\n",
    "        # These token vectors will be saved when in save_model(),\n",
    "        # but they should be ignored in load_model() as we want to use\n",
    "        # those computed using the current class names\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx:, :])  # CLS, EOS\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts  # torch.Tensor\n",
    "        self.name_lens = name_lens\n",
    "\n",
    "    def construct_prompts(self, ctx, prefix, suffix, label=None):\n",
    "        # dim0 is either batch_size (during training) or n_cls (during testing)\n",
    "        # ctx: context tokens, with shape of (dim0, n_ctx, ctx_dim)\n",
    "        # prefix: the sos token, with shape of (n_cls, 1, ctx_dim)\n",
    "        # suffix: remaining tokens, with shape of (n_cls, *, ctx_dim)\n",
    "\n",
    "        if label is not None:\n",
    "            prefix = prefix[label]\n",
    "            suffix = suffix[label]\n",
    "\n",
    "        prompts = torch.cat(\n",
    "            [\n",
    "                prefix,  # (dim0, 1, dim)\n",
    "                ctx,  # (dim0, n_ctx, dim)\n",
    "                suffix,  # (dim0, *, dim)\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return prompts\n",
    "\n",
    "    def forward(self, im_features):\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "        ctx = self.ctx  # (n_ctx, ctx_dim)\n",
    "\n",
    "\n",
    "\n",
    "        ############################################\n",
    "        ########## Q2,3. Fill in the blank #########\n",
    "        #bias = self.meta_net(\"Fill in here, Hint: Image feature is given as input to meta network\")  # (batch, ctx_dim)\n",
    "        bias = self.meta_net(im_features)  # (batch, ctx_dim)\n",
    "        bias = bias.unsqueeze(1)  # (batch, 1, ctx_dim)\n",
    "        ctx = ctx.unsqueeze(0)  # (1, n_ctx, ctx_dim)\n",
    "        #ctx_shifted = ctx + \"Fill in here, Hint: Add meta token to context token\"  # (batch, n_ctx, ctx_dim)\n",
    "        ctx_shifted = ctx + bias  # (batch, n_ctx, ctx_dim)\n",
    "        ############################################\n",
    "        ############################################\n",
    "\n",
    "\n",
    "\n",
    "        # Use instance-conditioned context tokens for all classes\n",
    "        prompts = []\n",
    "        for ctx_shifted_i in ctx_shifted:\n",
    "            ctx_i = ctx_shifted_i.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "            pts_i = self.construct_prompts(ctx_i, prefix, suffix)  # (n_cls, n_tkn, ctx_dim)\n",
    "            prompts.append(pts_i)\n",
    "        prompts = torch.stack(prompts)\n",
    "\n",
    "        return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "L_BluKEdKA94"
   },
   "outputs": [],
   "source": [
    "class CoCoOpCustomCLIP(nn.Module):\n",
    "    def __init__(self, cfg, classnames, clip_model):\n",
    "        super().__init__()\n",
    "        self.prompt_learner = CoCoOpPromptLearner(cfg, classnames, clip_model)\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, image, label=None):\n",
    "        tokenized_prompts = self.tokenized_prompts\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "\n",
    "        image_features = self.image_encoder(image.type(self.dtype))\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "        ############################################\n",
    "        ########## Q4. Fill in the blank #########\n",
    "        #prompts = self.prompt_learner(\"Fill in here\")\n",
    "        prompts = self.prompt_learner(image_features)\n",
    "        ############################################\n",
    "        ############################################\n",
    "\n",
    "\n",
    "        logits = []\n",
    "        for pts_i, imf_i in zip(prompts, image_features):\n",
    "            text_features = self.text_encoder(pts_i, tokenized_prompts)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "            l_i = logit_scale * imf_i @ text_features.t()\n",
    "            logits.append(l_i)\n",
    "        logits = torch.stack(logits)\n",
    "\n",
    "        if self.prompt_learner.training:\n",
    "            return F.cross_entropy(logits, label)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2CGZlqo-HtRN"
   },
   "source": [
    "### **Q2. Trainining CoCoOp**\n",
    "\n",
    "In this task, you will train CoCoOp on the EuroSAT dataset. If your implementation of CoCoOp in Question 1 is correct, the following code should execute without errors. Please submit the execution file so we can evaluate whether your code runs without any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Zy3bAMnBMrXP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trainer: CoCoOp\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from C:\\Users\\Jeongeun Park\\Downloads\\ProMetaR\\data\\eurosat\\split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from C:\\Users\\Jeongeun Park\\Downloads\\ProMetaR\\data\\eurosat\\split_fewshot\\shot_16-seed_1.pkl\n",
      "SUBSAMPLE BASE CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  5\n",
      "# train_x  80\n",
      "# val      20\n",
      "# test     4,200\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building custom CLIP\n",
      "Initial context: \"a photo of a\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.ctx', 'prompt_learner.meta_net.linear2.bias', 'prompt_learner.meta_net.linear1.bias', 'prompt_learner.meta_net.linear2.weight', 'prompt_learner.meta_net.linear1.weight'}\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m args\u001b[38;5;241m.\u001b[39msubsample_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m args\u001b[38;5;241m.\u001b[39meval_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m cocoop_base_acc \u001b[38;5;241m=\u001b[39m main(args)\n",
      "Cell \u001b[1;32mIn[4], line 426\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    423\u001b[0m     acc \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest()\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m acc\n\u001b[1;32m--> 426\u001b[0m acc \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m acc\n",
      "Cell \u001b[1;32mIn[4], line 376\u001b[0m, in \u001b[0;36mCoCoOp.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epoch):\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbefore_epoch()\n\u001b[1;32m--> 376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_epoch()\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_epoch()\n\u001b[0;32m    378\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_train()\n",
      "File \u001b[1;32m~\\Downloads\\ProMetaR\\dassl\\engine\\trainer.py:594\u001b[0m, in \u001b[0;36mTrainerX.run_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader_x)\n\u001b[0;32m    593\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader_x):\n\u001b[0;32m    595\u001b[0m     data_time\u001b[38;5;241m.\u001b[39mupdate(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m end)\n\u001b[0;32m    596\u001b[0m     loss_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_backward(batch)\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[0;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\multiprocessing\\connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\multiprocessing\\connection.py:346\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    344\u001b[0m             _winapi\u001b[38;5;241m.\u001b[39mPeekNamedPipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(wait([\u001b[38;5;28mself\u001b[39m], timeout))\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\multiprocessing\\connection.py:1084\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m   1081\u001b[0m                 ready_objects\u001b[38;5;241m.\u001b[39madd(o)\n\u001b[0;32m   1082\u001b[0m                 timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1084\u001b[0m     ready_handles \u001b[38;5;241m=\u001b[39m _exhaustive_wait(waithandle_to_obj\u001b[38;5;241m.\u001b[39mkeys(), timeout)\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m   1087\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\multiprocessing\\connection.py:1016\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m   1014\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[1;32m-> 1016\u001b[0m     res \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mWaitForMultipleObjects(L, \u001b[38;5;28;01mFalse\u001b[39;00m, timeout)\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train on the Base Classes Train split and evaluate accuracy on the Base Classes Test split.\n",
    "args.trainer = \"CoCoOp\"\n",
    "args.train_batch_size = 4\n",
    "args.epoch = 100\n",
    "args.output_dir = \"outputs/cocoop\"\n",
    "\n",
    "args.subsample_classes = \"base\"\n",
    "args.eval_only = False\n",
    "cocoop_base_acc = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Xql7WpJ5vPII"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trainer: CoCoOp\n",
      "Loading dataset: EuroSAT\n",
      "Reading split from C:\\Users\\Jeongeun Park\\Downloads\\ProMetaR\\data\\eurosat\\split_zhou_EuroSAT.json\n",
      "Loading preprocessed few-shot data from C:\\Users\\Jeongeun Park\\Downloads\\ProMetaR\\data\\eurosat\\split_fewshot\\shot_16-seed_1.pkl\n",
      "SUBSAMPLE NEW CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------\n",
      "Dataset    EuroSAT\n",
      "# classes  5\n",
      "# train_x  80\n",
      "# val      20\n",
      "# test     3,900\n",
      "---------  -------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Building custom CLIP\n",
      "Initial context: \"a photo of a\"\n",
      "Number of context words (tokens): 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.ctx', 'prompt_learner.meta_net.linear2.bias', 'prompt_learner.meta_net.linear1.bias', 'prompt_learner.meta_net.linear2.weight', 'prompt_learner.meta_net.linear1.weight'}\n",
      "Loading evaluator: Classification\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Model not found at \"outputs/cocoop\\prompt_learner\\model.pth.tar-100\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m args\u001b[38;5;241m.\u001b[39mload_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      6\u001b[0m args\u001b[38;5;241m.\u001b[39meval_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m coop_novel_acc \u001b[38;5;241m=\u001b[39m main(args)\n",
      "Cell \u001b[1;32mIn[4], line 422\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    420\u001b[0m trainer \u001b[38;5;241m=\u001b[39m build_trainer(cfg)\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39meval_only:\n\u001b[1;32m--> 422\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mload_model(args\u001b[38;5;241m.\u001b[39mmodel_dir, epoch\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mload_epoch)\n\u001b[0;32m    423\u001b[0m     acc \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest()\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m acc\n",
      "Cell \u001b[1;32mIn[4], line 333\u001b[0m, in \u001b[0;36mCoCoOp.load_model\u001b[1;34m(self, directory, epoch)\u001b[0m\n\u001b[0;32m    330\u001b[0m model_path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(directory, name, model_file)\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m osp\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel not found at \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_path))\n\u001b[0;32m    335\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m load_checkpoint(model_path)\n\u001b[0;32m    336\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Model not found at \"outputs/cocoop\\prompt_learner\\model.pth.tar-100\""
     ]
    }
   ],
   "source": [
    "# Accuracy on the New Classes.\n",
    "args.model_dir = \"outputs/cocoop\"\n",
    "args.output_dir = \"outputs/cocoop/new_classes\"\n",
    "args.subsample_classes = \"new\"\n",
    "args.load_epoch = 100\n",
    "args.eval_only = True\n",
    "coop_novel_acc = main(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "l1KdgiKFsowj"
   },
   "source": [
    "### **Q3. Analyzing the results of CoCoOp**\n",
    "Compare the results of CoCoOp with those of CoOp that we trained in Lab Session 4. Discuss possible reasons for the performance differences observed between CoCoOp and CoOp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비록 learn2learn 패키지 다운로드에 대한 오류로 인하여 결과가 나오지 않았으나, CoCoOp and CoOp를 비교하도록 하겠다\n",
    "\n",
    "#### 1. CoCoOp의 Dynamic Adaptation\n",
    "CoCoOp은 input image features에 따라 context embeddings을 동적으로 학습하는 방식으로, 보다 유연한 접근 방식을 사용한다. 이는 각 instance에 맞춰 특정한 context tokens을 생성하게 하여, 다양한 visual content의 변화를 더욱 효과적으로 반영할 수 있게 한다. 반면 CoOp은 fixed context embeddings을 모든 instance에 동일하게 적용하기 때문에, 각 image의 개별 특성을 충분히 포착하지 못할 수 있다.\n",
    "\n",
    "#### 2. Improved Generalization\n",
    "CoCoOp은 조건부로 context embeddings을 생성하므로, fixed embeddings을 사용하는 CoOp에 비해 새로운 데이터나 클래스에 대해 더 나은 generalization 성능을 보인다. CoCoOp의 context embeddings은 각 instance에 맞춰 유연하게 조정되므로, 예측에 필요한 미세한 패턴을 잘 반영할 수 있다. 반면, CoOp의 fixed embeddings은 data에 나타나는 다양한 변화를 포착하지 못해 unseen data에서 한계가 있을 수 있다.\n",
    "\n",
    "#### 3. Out-of-Distribution (OOD) 데이터에 대한 Performance Differences\n",
    "CoCoOp은 out-of-distribution (OOD) 데이터에서 더 나은 성능을 보인다. 이는 CoCoOp이 다양한 input contexts에 적응할 수 있어 data distribution이 달라질 때도 유연하게 대응할 수 있기 때문이다. CoOp은 fixed context tokens을 사용하기 때문에, OOD 데이터에서의 performance drop이 더 클 수 있다.\n",
    "\n",
    "#### 4. Computational Complexity\n",
    "CoCoOp은 각 image에 대해 동적으로 context embeddings을 생성하는 meta-network를 사용하기 때문에 추가적인 computational overhead가 발생한다. 이는 CoOp이 pre-computed fixed embeddings을 사용하는 것과 비교하여 inference 속도가 약간 느려질 수 있다.\n",
    "\n",
    "#### 5. Possible Reasons for Performance Differences\n",
    "\n",
    "Context Flexibility: CoCoOp은 input에 따라 context embeddings을 조정할 수 있는 능력 덕분에, 다양한 dataset에서 유리하게 작용할 수 있다.\n",
    "Overfitting in CoOp: CoOp의 fixed context embeddings은 training data에 과적합(overfitting)될 가능성이 있으며, 변화에 적응하지 못할 수 있다. CoCoOp은 각 instance에 맞춰 context를 조정함으로써 이러한 과적합 위험을 완화한다.\n",
    "Representation Power: CoCoOp은 instance에 따라 다르게 생성된 context tokens을 사용하여 보다 풍부하고 표현력 있는 특징을 학습할 수 있으며, 이는 복잡하거나 모호한 상황에서 더 나은 의사결정을 가능하게 한다."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "G3n9blo4JO7m",
    "2CGZlqo-HtRN"
   ],
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
